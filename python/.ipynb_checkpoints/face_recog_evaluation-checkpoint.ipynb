{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import cv2\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import sklearn\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = face_recognition.load_image_file('/home/huy/data/face-dataset/colorferet/00001/00001_930831_fa_a.ppm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 256, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(139, 196, 268, 67)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_recognition.face_locations(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x[139:268,67:196,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129, 129, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = cv2.resize(y, (92,112))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 92, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow('test', y)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9099079a335b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdowns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_image_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbottom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mRESIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Our Database\n",
    "LOCATION = '/home/huy/code/godofeye/train_data/all'\n",
    "\n",
    "# Gray scale image\n",
    "GRAY_SCALE = True\n",
    "RESIZE = True\n",
    "\n",
    "TRAIN_SET_FILES = []\n",
    "TRAIN_LABEL= []\n",
    "TEST_SET_FILES = []\n",
    "TEST_LABEL = []\n",
    "\n",
    "TRAIN_SET = []\n",
    "TEST_SET = []\n",
    "TEST_SET_LEFT_FILES = []\n",
    "TEST_SET_LEFT = []\n",
    "TEST_LEFT_LABEL = []\n",
    "TEST_SET_RIGHT_FILES = []\n",
    "TEST_SET_RIGHT = []\n",
    "TEST_RIGHT_LABEL = []\n",
    "TEST_SET_UP_FILES = []\n",
    "TEST_SET_UP = []\n",
    "TEST_UP_LABEL = []\n",
    "TEST_SET_DOWN_FILES = []\n",
    "TEST_SET_DOWN = []\n",
    "TEST_DOWN_LABEL = []\n",
    "\n",
    "TEST_LEFT_LABEL_TEMP = []\n",
    "TEST_RIGHT_LABEL_TEMP = []\n",
    "TEST_UP_LABEL_TEMP = []\n",
    "TEST_DOWN_LABEL_TEMP = []\n",
    "\n",
    "DB_SUMMARY = pd.DataFrame(columns=['Name', 'Center', 'Left', 'Right', 'Up', 'Down'])\n",
    "\n",
    "for entry in os.scandir(LOCATION):\n",
    "    if entry.is_dir():\n",
    "        types = ['*.jpg', '*.png']\n",
    "        files = []\n",
    "        for t in types:\n",
    "            files.extend(glob.glob(os.path.join(entry.path, t)))\n",
    "        centers = [path for path in files if 'center' in path]\n",
    "        ups = [path for path in files if 'up' in path]\n",
    "        downs = [path for path in files if 'down' in path]\n",
    "        lefts = [path for path in files if 'left' in path]\n",
    "        rights = [path for path in files if 'right' in path]\n",
    "        if len(centers) == 0:\n",
    "            print(entry.name)\n",
    "            \n",
    "        total_pose = [len(centers)-1, len(lefts), len(rights), len(ups), len(downs)]\n",
    "        detected_count = [0, 0, 0, 0, 0]\n",
    "        # encode 1 train image\n",
    "        image = face_recognition.load_image_file(centers[0])\n",
    "        try:\n",
    "            (top, right, bottom, left) = face_recognition.face_locations(image)[0]\n",
    "            image = image[top:bottom,left:right,:]\n",
    "            if RESIZE:\n",
    "                image = cv2.resize(image, (92,112))\n",
    "            if GRAY_SCALE:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "            image_box = (0, image.shape[1], image.shape[0], 0)\n",
    "            encoded = face_recognition.face_encodings(image, known_face_locations=[image_box])[0]\n",
    "            TRAIN_SET.append(encoded)\n",
    "            TRAIN_LABEL.append(entry.name)\n",
    "        except:\n",
    "            print('Cant find face from image in train set', centers[0])\n",
    "        \n",
    "        for path in centers[1:]:\n",
    "            try:\n",
    "                image = face_recognition.load_image_file(path)\n",
    "                (top, right, bottom, left) = face_recognition.face_locations(image)[0]\n",
    "                image = image[top:bottom,left:right,:]\n",
    "                if RESIZE:\n",
    "                    image = cv2.resize(image, (92,112))\n",
    "                if GRAY_SCALE:\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "                image_box = (0, image.shape[1], image.shape[0], 0)\n",
    "                encoded = face_recognition.face_encodings(image, known_face_locations=[image_box])[0]\n",
    "                TEST_SET.append(encoded)\n",
    "                TEST_LABEL.append(entry.name)\n",
    "                detected_count[0] += 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print('Cant find face from image in center set', path)\n",
    "        for path in lefts:\n",
    "            try:\n",
    "                image = face_recognition.load_image_file(path)\n",
    "                (top, right, bottom, left) = face_recognition.face_locations(image)[0]\n",
    "                image = image[top:bottom,left:right,:]\n",
    "                if RESIZE:\n",
    "                    image = cv2.resize(image, (92,112))\n",
    "                if GRAY_SCALE:\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "                image_box = (0, image.shape[1], image.shape[0], 0)\n",
    "                encoded = face_recognition.face_encodings(image, known_face_locations=[image_box])[0]\n",
    "                TEST_SET_LEFT.append(encoded)\n",
    "                TEST_LEFT_LABEL.append(entry.name)\n",
    "                detected_count[1] += 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print('Cant find face from image in left set', path)\n",
    "        \n",
    "        for path in rights:\n",
    "            try:\n",
    "                image = face_recognition.load_image_file(path)\n",
    "                (top, right, bottom, left) = face_recognition.face_locations(image)[0]\n",
    "                image = image[top:bottom,left:right,:]\n",
    "                if RESIZE:\n",
    "                    image = cv2.resize(image, (92,112))\n",
    "                if GRAY_SCALE:\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "                image_box = (0, image.shape[1], image.shape[0], 0)\n",
    "                encoded = face_recognition.face_encodings(image, known_face_locations=[image_box])[0]\n",
    "                TEST_SET_RIGHT.append(encoded)\n",
    "                TEST_RIGHT_LABEL.append(entry.name)\n",
    "                detected_count[2] += 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print('Cant find face from image in right set', path)\n",
    "        \n",
    "        for path in ups:\n",
    "            try:\n",
    "                image = face_recognition.load_image_file(path)\n",
    "                (top, right, bottom, left) = face_recognition.face_locations(image)[0]\n",
    "                image = image[top:bottom,left:right,:]\n",
    "                if RESIZE:\n",
    "                    image = cv2.resize(image, (92,112))\n",
    "                if GRAY_SCALE:\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "                image_box = (0, image.shape[1], image.shape[0], 0)\n",
    "                encoded = face_recognition.face_encodings(image, known_face_locations=[image_box])[0]\n",
    "                TEST_SET_UP.append(encoded)\n",
    "                TEST_UP_LABEL.append(entry.name)\n",
    "                detected_count[3] += 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print('Cant find face from image in left set', path)\n",
    "        \n",
    "        for path in downs:\n",
    "            try:\n",
    "                image = face_recognition.load_image_file(path)\n",
    "                (top, right, bottom, left) = face_recognition.face_locations(image)[0]\n",
    "                image = image[top:bottom,left:right,:]\n",
    "                if RESIZE:\n",
    "                    image = cv2.resize(image, (92,112))\n",
    "                if GRAY_SCALE:\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "                image_box = (0, image.shape[1], image.shape[0], 0)\n",
    "                encoded = face_recognition.face_encodings(image, known_face_locations=[image_box])[0]\n",
    "                TEST_SET_DOWN.append(encoded)\n",
    "                TEST_DOWN_LABEL.append(entry.name)\n",
    "                detected_count[4] += 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print('Cant find face from image in left set', path)\n",
    "        detect_per_total = [f'{x}/{y}' for x,y in zip(detected_count,total_pose)]\n",
    "        row_to_append = [entry.name, *detect_per_total]\n",
    "        DB_SUMMARY = DB_SUMMARY.append(pd.Series(row_to_append, index=DB_SUMMARY.columns), ignore_index=True)\n",
    "        #TRAIN_SET_FILES.append(centers[0])\n",
    "        #TRAIN_LABEL.append(entry.name)\n",
    "        \n",
    "        #TEST_SET_FILES.extend(centers[1:])\n",
    "        #TEST_LABEL.extend([entry.name for _ in range(len(centers) - 1)])\n",
    "        \n",
    "        #TEST_SET_LEFT_FILES.extend(lefts)\n",
    "        #TEST_LEFT_LABEL_TEMP.extend([entry.name for _ in range(len(lefts))])\n",
    "        \n",
    "        #TEST_SET_RIGHT_FILES.extend(rights)\n",
    "        #TEST_RIGHT_LABEL_TEMP.extend([entry.name for _ in range(len(rights))])\n",
    "        \n",
    "        #TEST_SET_UP_FILES.extend(ups)\n",
    "        #TEST_UP_LABEL_TEMP.extend([entry.name for _ in range(len(ups))])\n",
    "        \n",
    "        #TEST_SET_DOWN_FILES.extend(downs)\n",
    "        #TEST_DOWN_LABEL_TEMP.extend([entry.name for _ in range(len(downs))])\n",
    "        \n",
    "        \n",
    "# for file in TRAIN_SET_FILES:\n",
    "#     image = face_recognition.load_image_file(file)\n",
    "#     try:\n",
    "#         encoded = face_recognition.face_encodings(image)[0]\n",
    "#         TRAIN_SET.append(encoded)\n",
    "#     except:\n",
    "#         print('Cant find face from image in train set', file)\n",
    "\n",
    "# for file in TEST_SET_FILES:\n",
    "#     image = face_recognition.load_image_file(file)\n",
    "#     try:\n",
    "#         encoded = face_recognition.face_encodings(image)[0]\n",
    "#         TEST_SET.append(encoded)\n",
    "#     except:\n",
    "#         print('Cant find face from image in test set', file)\n",
    "# for loc, file in enumerate(TEST_SET_LEFT_FILES):\n",
    "#     image = face_recognition.load_image_file(file)\n",
    "#     try:\n",
    "#         encoded = face_recognition.face_encodings(image)[0]\n",
    "#         TEST_SET_LEFT.append(encoded)\n",
    "#         TEST_LEFT_LABEL.append(TEST_LEFT_LABEL_TEMP[loc])\n",
    "#     except:\n",
    "#         print('Cant find face from image in test set', file)\n",
    "        \n",
    "# for loc, file in enumerate(TEST_SET_RIGHT_FILES):\n",
    "#     image = face_recognition.load_image_file(file)\n",
    "#     try:\n",
    "#         encoded = face_recognition.face_encodings(image)[0]\n",
    "#         TEST_SET_RIGHT.append(encoded)\n",
    "#         TEST_RIGHT_LABEL.append(TEST_RIGHT_LABEL_TEMP[loc])\n",
    "#     except:\n",
    "#         print('Cant find face from image in test set', file)\n",
    "        \n",
    "# for loc, file in enumerate(TEST_SET_UP_FILES):\n",
    "#     image = face_recognition.load_image_file(file)\n",
    "#     try:\n",
    "#         encoded = face_recognition.face_encodings(image)[0]\n",
    "#         TEST_SET_UP.append(encoded)\n",
    "#         TEST_UP_LABEL.append(TEST_UP_LABEL_TEMP[loc])\n",
    "#     except:\n",
    "#         print('Cant find face from image in test set', file)\n",
    "        \n",
    "# for loc, file in enumerate(TEST_SET_DOWN_FILES):\n",
    "#     image = face_recognition.load_image_file(file)\n",
    "#     try:\n",
    "#         encoded = face_recognition.face_encodings(image)[0]\n",
    "#         TEST_SET_DOWN.append(encoded)\n",
    "#         TEST_DOWN_LABEL.append(TEST_DOWN_LABEL_TEMP[loc])\n",
    "#     except:\n",
    "#         print('Cant find face from image in test set', file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AT&T Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AT&T\n",
    "LOCATION = '/home/huy/data/face-dataset/att_faces/orl_faces_labeled'\n",
    "TRAIN_SET_FILES = []\n",
    "TRAIN_LABEL= []\n",
    "TEST_SET_FILES = []\n",
    "TEST_LABEL = []\n",
    "\n",
    "TRAIN_SET = []\n",
    "TEST_SET = []\n",
    "\n",
    "TRAIN_SET_LEFT_FILES = []\n",
    "TRAIN_SET_LEFT = []\n",
    "TRAIN_LEFT_LABEL = []\n",
    "TRAIN_SET_RIGHT_FILES = []\n",
    "TRAIN_SET_RIGHT = []\n",
    "TRAIN_RIGHT_LABEL = []\n",
    "TRAIN_SET_UP_FILES = []\n",
    "TRAIN_SET_UP = []\n",
    "TRAIN_UP_LABEL = []\n",
    "TRAIN_SET_DOWN_FILES = []\n",
    "TRAIN_SET_DOWN = []\n",
    "TRAIN_DOWN_LABEL = []\n",
    "\n",
    "TEST_SET_LEFT_FILES = []\n",
    "TEST_SET_LEFT = []\n",
    "TEST_LEFT_LABEL = []\n",
    "TEST_SET_RIGHT_FILES = []\n",
    "TEST_SET_RIGHT = []\n",
    "TEST_RIGHT_LABEL = []\n",
    "TEST_SET_UP_FILES = []\n",
    "TEST_SET_UP = []\n",
    "TEST_UP_LABEL = []\n",
    "TEST_SET_DOWN_FILES = []\n",
    "TEST_SET_DOWN = []\n",
    "TEST_DOWN_LABEL = []\n",
    "\n",
    "TEST_LEFT_LABEL_TEMP = []\n",
    "TEST_RIGHT_LABEL_TEMP = []\n",
    "TEST_UP_LABEL_TEMP = []\n",
    "TEST_DOWN_LABEL_TEMP = []\n",
    "\n",
    "DB_SUMMARY = pd.DataFrame(columns=['Name', 'Center', 'Left', 'Right', 'Up', 'Down'])\n",
    "\n",
    "for entry in os.scandir(LOCATION):\n",
    "    if entry.is_dir() and len(entry.name) < 4:\n",
    "        types = ['*.jpg', '*.png', '*.pgm']\n",
    "        files = []\n",
    "        for t in types:\n",
    "            files.extend(glob.glob(os.path.join(entry.path, t)))\n",
    "        centers = [path for path in files if 'center' in path]\n",
    "        ups = [path for path in files if 'up' in path]\n",
    "        downs = [path for path in files if 'down' in path]\n",
    "        lefts = [path for path in files if 'left' in path]\n",
    "        rights = [path for path in files if 'right' in path]\n",
    "        if len(centers) == 0:\n",
    "            print(entry.name)\n",
    "            \n",
    "        total_pose = [len(centers)-1, len(lefts), len(rights), len(ups), len(downs)]\n",
    "        detected_count = [0, 0, 0, 0, 0]\n",
    "        # encode 1 train image\n",
    "        image = face_recognition.load_image_file(centers[0])\n",
    "        try:\n",
    "            encoded = face_recognition.face_encodings(image, known_face_locations=[(0,image.shape[1],image.shape[0],0)])[0]\n",
    "            TRAIN_SET.append(encoded)\n",
    "            TRAIN_LABEL.append(entry.name)\n",
    "        except:\n",
    "            print('Cant find face from image in train set', centers[0])\n",
    "        \n",
    "        for path in centers[1:]:\n",
    "            image = face_recognition.load_image_file(path)\n",
    "            try:\n",
    "                encoded = face_recognition.face_encodings(image, known_face_locations=[(0,image.shape[1],image.shape[0],0)])[0]\n",
    "                TEST_SET.append(encoded)\n",
    "                TEST_LABEL.append(entry.name)\n",
    "                detected_count[0] += 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print('Cant find face from image in center set', path)\n",
    "        for path in lefts:\n",
    "            image = face_recognition.load_image_file(path)\n",
    "            try:\n",
    "                encoded = face_recognition.face_encodings(image, known_face_locations=[(0,image.shape[1],image.shape[0],0)])[0]\n",
    "                TEST_SET_LEFT.append(encoded)\n",
    "                TEST_LEFT_LABEL.append(entry.name)\n",
    "                detected_count[1] += 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print('Cant find face from image in left set', path)\n",
    "        \n",
    "        for path in rights:\n",
    "            image = face_recognition.load_image_file(path)\n",
    "            try:\n",
    "                encoded = face_recognition.face_encodings(image, known_face_locations=[(0,image.shape[1],image.shape[0],0)])[0]\n",
    "                TEST_SET_RIGHT.append(encoded)\n",
    "                TEST_RIGHT_LABEL.append(entry.name)\n",
    "                detected_count[2] += 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print('Cant find face from image in right set', path)\n",
    "        \n",
    "        for path in ups:\n",
    "            image = face_recognition.load_image_file(path)\n",
    "            try:\n",
    "                encoded = face_recognition.face_encodings(image, known_face_locations=[(0,image.shape[1],image.shape[0],0)])[0]\n",
    "                TEST_SET_UP.append(encoded)\n",
    "                TEST_UP_LABEL.append(entry.name)\n",
    "                detected_count[3] += 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print('Cant find face from image in left set', path)\n",
    "        \n",
    "        for path in downs:\n",
    "            image = face_recognition.load_image_file(path)\n",
    "            try:\n",
    "                encoded = face_recognition.face_encodings(image, known_face_locations=[(0,image.shape[1],image.shape[0],0)])[0]\n",
    "                TEST_SET_DOWN.append(encoded)\n",
    "                TEST_DOWN_LABEL.append(entry.name)\n",
    "                detected_count[4] += 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print('Cant find face from image in left set', path) \n",
    "        detect_per_total = [f'{x}/{y}' for x,y in zip(detected_count,total_pose)]\n",
    "        row_to_append = [entry.name, *detect_per_total]\n",
    "        DB_SUMMARY = DB_SUMMARY.append(pd.Series(row_to_append, index=DB_SUMMARY.columns), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AT face dataset\n",
    "os.chdir('/home/huy/data/face-dataset/att_faces/orl_faces/')\n",
    "train_set = []\n",
    "train_set_label = []\n",
    "\n",
    "test_set = []\n",
    "test_set_label = []\n",
    "\n",
    "test_set_hm  = []\n",
    "test_label_hm = []\n",
    "\n",
    "test_filename = [str(x) + '.pgm' for x in range(2,11)]\n",
    "#test_file_ext_hm = ['.happy0.png', '.surprised0.png', '.wink0.png', '.leftlight0.png', '.rightlight0.png', '.centerlight0.png']\n",
    "\n",
    "\n",
    "for entry in os.scandir('.'):\n",
    "    if entry.is_dir() == True:\n",
    "        label = re.findall('s(.+)', entry.name)[0]\n",
    "        if entry.name not in ['s36', 's37', 's38', 's39', 's40'] :\n",
    "            train_set_label.append(label)\n",
    "            image_filename = '1.pgm'\n",
    "            image = face_recognition.load_image_file(os.path.join(entry.path, image_filename))\n",
    "            encoded = face_recognition.face_encodings(image)[0]\n",
    "            train_set.append(encoded)\n",
    "        \n",
    "        for file in test_filename:\n",
    "            if int(label) >= 36:\n",
    "                label = '36'\n",
    "            img = face_recognition.load_image_file(os.path.join(entry.path, file))\n",
    "            try:\n",
    "                h, w, _ = img.shape\n",
    "                roi = [(0,w,h,0)]\n",
    "                encoded = face_recognition.face_encodings(img, known_face_locations=roi)[0]\n",
    "            except :\n",
    "                print(img_filename)\n",
    "            test_set.append(encoded)\n",
    "            test_set_label.append(label)\n",
    "#         for ext in test_file_ext_hm:\n",
    "#             img_filename = entry.name + ext\n",
    "#             img = face_recognition.load_image_file(os.path.join(entry.path, img_filename))\n",
    "#             try:\n",
    "#                 h, w, _ = img.shape\n",
    "#                 roi = [(0,w,h,0)]\n",
    "#                 encoded = face_recognition.face_encodings(img, known_face_locations=roi)[0]\n",
    "#             except :\n",
    "#                 print(img_filename)\n",
    "#             test_set_hm.append(encoded)\n",
    "#             test_label_hm.append(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YALE B dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/huy/Downloads/face-dataset/YALE/faceonly/')\n",
    "\n",
    "train_set = []\n",
    "train_set_label = []\n",
    "\n",
    "test_set = []\n",
    "test_set_label = []\n",
    "\n",
    "test_set_hm  = []\n",
    "test_set_label_hm = []\n",
    "\n",
    "test_file_ext = ['.glasses0.png', '.sleepy0.png', '.noglasses0.png', '.sad0.png']\n",
    "test_file_ext_hm = ['.happy0.png', '.surprised0.png', '.wink0.png', '.leftlight0.png', '.rightlight0.png', '.centerlight0.png']\n",
    "\n",
    "for entry in os.scandir('.'):\n",
    "    if entry.is_dir() == True:\n",
    "        label = re.findall('subject(.+)', entry.name)[0]\n",
    "        if entry.name != 'subject15':\n",
    "            train_set_label.append(label)\n",
    "            image_filename = entry.name + '.normal0.png'\n",
    "            image = face_recognition.load_image_file(os.path.join(entry.path, image_filename))\n",
    "            encoded = face_recognition.face_encodings(image)[0]\n",
    "            train_set.append(encoded)\n",
    "        \n",
    "        for ext in test_file_ext:\n",
    "            img_filename = entry.name + ext\n",
    "            img = face_recognition.load_image_file(os.path.join(entry.path, img_filename))\n",
    "            encoded = face_recognition.face_encodings(img)[0]\n",
    "            test_set.append(encoded)\n",
    "            test_set_label.append(label)\n",
    "        for ext in test_file_ext_hm:\n",
    "            img_filename = entry.name + ext\n",
    "            img = face_recognition.load_image_file(os.path.join(entry.path, img_filename))\n",
    "            try:\n",
    "                h, w, _ = img.shape\n",
    "                roi = [(0,w,h,0)]\n",
    "                encoded = face_recognition.face_encodings(img, known_face_locations=roi)[0]\n",
    "            except :\n",
    "                print(img_filename)\n",
    "            test_set_hm.append(encoded)\n",
    "            test_set_label_hm.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTENDED YALE B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import acos, cos, degrees, radians\n",
    "# row - azimuth\n",
    "# column - elevation\n",
    "LOCATION = '/home/huy/data/face-dataset/ExtendedYaleB'\n",
    "\n",
    "# azimuths = [-130, -120, -110, -95, -85, -70, -60, -50, -35, -25, -20, -15, -10, -5, 0, 5, 10, 15, 20, 25, 35, 50, 60, 70, 85, 95, 110, 120, 130]\n",
    "# elevations = [-40, -35, -20, -10, 0, 10, 15, 20, 40, 45, 65, 90]\n",
    "angle_range = [(0,12), (12,25), (25,50), (50,77), (77,130)]\n",
    "# df = pd.DataFrame([[0]*len(elevations)]*len(azimuths),index=azimuths, columns=elevations)\n",
    "\n",
    "TRAIN_SET = []\n",
    "TRAIN_LABEL = []\n",
    "\n",
    "TEST_SET_LIST = [[] for _ in range(5)]\n",
    "TEST_LABEL_LIST = [[] for _ in range(5)]\n",
    "\n",
    "error_files = []\n",
    "count = 0\n",
    "\n",
    "for entry in os.scandir(LOCATION):\n",
    "    if entry.is_dir():\n",
    "        for path in glob.glob(os.path.join(entry.path, '*.pgm')):\n",
    "            filename = os.path.basename(path)\n",
    "            m = re.search(r\"P00A(.+)E(.+)\\.\", filename)\n",
    "            if m:\n",
    "                azimuth, elevation = list(map(int,m.groups()))\n",
    "#                 df.loc[azimuth][elevation] += 1\n",
    "                c_angle = degrees(acos(cos(radians(azimuth))*cos(radians(elevation))))\n",
    "                img = face_recognition.load_image_file(os.path.join(entry.path, path))\n",
    "                try:\n",
    "                    encoded = face_recognition.face_encodings(img)[0]\n",
    "                    if c_angle == 0:\n",
    "                        TRAIN_SET.append(encoded)\n",
    "                        TRAIN_LABEL.append(entry.name)\n",
    "                    for i,t in enumerate(angle_range):\n",
    "                        min_angle, max_angle = t\n",
    "                        if c_angle > min_angle and c_angle <= max_angle:\n",
    "                            TEST_SET_LIST[i].append(encoded)\n",
    "                            TEST_LABEL_LIST[i].append(entry.name)\n",
    "                            break\n",
    "                except :\n",
    "                    error_files.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEST_SET_LIST[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SET_LIST = [[] for _ in range(5)]\n",
    "TEST_LABEL_LIST = [[]]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SET_LIST[0].append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_SET_LIST[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COLOR FERET dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/huy/data/face-dataset/colorferet/')\n",
    "\n",
    "train_set = []\n",
    "train_set_label = []\n",
    "test_set = []\n",
    "test_set_label = []\n",
    "\n",
    "train_file_ext = ['fa']\n",
    "\n",
    "test_file_ext = ['fb', 'fb_']\n",
    "\n",
    "dataset_summary = {}\n",
    "\n",
    "for entry in os.scandir('.'):\n",
    "    train_sample_count = [0 for _ in range(len(train_file_ext))]\n",
    "    file_count = len(os.listdir(entry.path))\n",
    "    dataset_summary.update({entry.name : file_count})\n",
    "    for file in os.scandir(entry.path):\n",
    "        for i, ext in enumerate(train_file_ext):\n",
    "            if int(entry.name) < 1000:\n",
    "                if ext in file.name:\n",
    "                    if train_sample_count[i] < 1:\n",
    "                        try:\n",
    "                            im = face_recognition.load_image_file(file.path)\n",
    "                            encoded_face = face_recognition.face_encodings(im)[0]\n",
    "                            train_set.append(encoded_face)\n",
    "                            train_set_label.append(entry.name)\n",
    "                        except:\n",
    "                            print(file.name)\n",
    "                    train_sample_count[i] += 1\n",
    "        \n",
    "#     if train_sample_count[0] == 0:\n",
    "#         print(entry.name)\n",
    "#     folder_count += 1\n",
    "# print(count)\n",
    "        all posible image per person in test_set\n",
    "        for i, ext in enumerate(test_file_ext):\n",
    "            if ext in file.name:\n",
    "                im = face_recognition.load_image_file(file.path)\n",
    "                try:\n",
    "                    encoded_face = face_recognition.face_encodings(im)[0]\n",
    "                    test_set.append(encoded_face)\n",
    "                    if int(entry.name) < 1000:\n",
    "                        test_set_label.append(entry.name)\n",
    "                    else:\n",
    "                        test_set_label.append('01000')\n",
    "                except:\n",
    "                    print(file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLOR FERET dataset\n",
    "os.chdir('/home/huy/data/face-dataset/colorferet/')\n",
    "\n",
    "train_set = []\n",
    "train_set_label = []\n",
    "test_set = []\n",
    "test_set_label = []\n",
    "\n",
    "train_file_ext = ['fa']\n",
    "\n",
    "test_file_ext = ['fb', 'fb_']\n",
    "\n",
    "dataset_summary = {}\n",
    "\n",
    "# train_images = []\n",
    "# test_images = []\n",
    "# labels = []\n",
    "person_count = 0\n",
    "for entry in os.scandir('.'):\n",
    "    test_count = 0\n",
    "    train_count = 0\n",
    "    if entry.is_dir():\n",
    "        print('Processing', entry.name)\n",
    "        person_count += 1\n",
    "        print(person_count)\n",
    "        for path in glob.glob(os.path.join(entry.path,'*.ppm')):\n",
    "            filename = os.path.basename(path)\n",
    "            if 'fa' in filename and train_count == 0:\n",
    "                train_count = 1\n",
    "                try:\n",
    "                    im = face_recognition.load_image_file(path)\n",
    "                    encoded_face = face_recognition.face_encodings(im)[0]\n",
    "                    train_set.append(encoded_face)\n",
    "                    train_set_label.append(entry.name)\n",
    "                except:\n",
    "                    print('Train',filename)\n",
    "            elif 'fb' in filename or 'hl' in filename or 'hr' in filename:\n",
    "#                 test_count = 1\n",
    "                try:\n",
    "                    im = face_recognition.load_image_file(path)\n",
    "                    encoded_face = face_recognition.face_encodings(im)[0]\n",
    "                    test_set.append(encoded_face)\n",
    "                    test_set_label.append(entry.name)\n",
    "                except:\n",
    "                    print('Test', filename)\n",
    "        if person_count == 50:\n",
    "            print('Make Dataset 50')\n",
    "            train_50 = train_set.copy()\n",
    "            train_50_label = train_set_label.copy()\n",
    "            test_50 = test_set.copy()\n",
    "            test_50_label = test_set_label.copy()\n",
    "        elif person_count == 500:\n",
    "            print('Make Dataset 500')\n",
    "            train_500 = train_set.copy()\n",
    "            train_500_label = train_set_label.copy()\n",
    "            test_500 = test_set.copy()\n",
    "            test_500_label = test_set_label.copy()\n",
    "        elif person_count == 993:\n",
    "            print('Make Dataset 1000')\n",
    "            train_1000 = train_set.copy()\n",
    "            train_1000_label = train_set_label.copy()\n",
    "            test_1000 = test_set.copy()\n",
    "            test_1000_label = test_set_label.copy()\n",
    "            \n",
    "#         train_sample_count = [0 for _ in range(len(train_file_ext))]\n",
    "# #         file_count = len(os.listdir(entry.path))\n",
    "# #         dataset_summary.update({entry.name : file_count})\n",
    "#         train_image_count = 0\n",
    "#         test_image_count = 0\n",
    "#         TEST_IMAGE_MAX = 1\n",
    "        \n",
    "#         for file in os.scandir(entry.path):\n",
    "#             for i, ext in enumerate(train_file_ext):\n",
    "#                 if ext in file.name:\n",
    "#                     if train_sample_count[i] < 1:\n",
    "#                         train_images.append(file.path)\n",
    "#                         train_sample_count[i] += 1\n",
    "#             for i, ext in enumerate(test_file_ext):\n",
    "#                 if ext in file.name:\n",
    "#                     if test_image_count < TEST_IMAGE_MAX:\n",
    "#                         test_images.append(file.path)\n",
    "#                         test_image_count += 1\n",
    "#         if train_images and test_images:\n",
    "#             for train_image in train_images:\n",
    "#                 try:\n",
    "#                     im = face_recognition.load_image_file(train_image)\n",
    "#                     encoded_face = face_recognition.face_encodings(im)[0]\n",
    "#                     train_set.append(encoded_face)\n",
    "#                     train_set_label.append(entry.name)\n",
    "#                 except:\n",
    "#                     print(file.name)\n",
    "#             for test_image in test_images:\n",
    "#                 try:\n",
    "#                     im = face_recognition.load_image_file(test_image)\n",
    "#                     encoded_face = face_recognition.face_encodings(im)[0]\n",
    "#                     test_set.append(encoded_face)\n",
    "#                     test_set_label.append(entry.name)\n",
    "#                 except:\n",
    "#                     print(file.name)\n",
    "#             labels.append(entry.name)\n",
    "#         else:\n",
    "#             print('Train or test image not found', entry.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_100 = test_set[0:50]\n",
    "# test_100_label = test_set_label[0:50]\n",
    "# test_500 = test_set[50:550]\n",
    "# test_500_label = test_set_label[50:550]\n",
    "# test_1000 = test_set\n",
    "# test_1000_label = test_set_label\n",
    "\n",
    "# train_100 = train_set[0:50]\n",
    "# train_100_label = train_set_label[0:50]\n",
    "# train_500 = train_set[50:550]\n",
    "# train_500_label = train_set_label[50:550]\n",
    "# train_1000 = train_set\n",
    "# train_1000_label = train_set_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to different number of images\n",
    "train_set_list = []\n",
    "train_labels_list = []\n",
    "test_set_list = []\n",
    "test_labels_list = []\n",
    "for scale in range(1,101):\n",
    "    k = 10*scale\n",
    "    if k > 990:\n",
    "        k = 992\n",
    "    features, labels = zip(*[(train_1000[i], train_1000_label[i]) for i in random.sample(range(992), k)])\n",
    "    features = list(features)\n",
    "    labels = list(labels)\n",
    "    train_set_list.append(features)\n",
    "    train_labels_list.append(labels)\n",
    "    temp_test_set = []\n",
    "    temp_test_labels = []\n",
    "    for i in range(len(test_1000)):\n",
    "        if test_1000_label[i] in labels:\n",
    "            temp_test_set.append(test_1000[i])\n",
    "            temp_test_labels.append(test_1000_label[i])\n",
    "    test_set_list.append(temp_test_set)\n",
    "    test_labels_list.append(temp_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle FERET dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('feret_train_50.pickle', 'wb') as file:\n",
    "    pickle.dump({'features':train_50, 'labels':train_50_label}, file)\n",
    "with open('feret_train_500.pickle', 'wb') as file:\n",
    "    pickle.dump({'features':train_500, 'labels':train_500_label}, file)\n",
    "with open('feret_train_1000.pickle', 'wb') as file:\n",
    "    pickle.dump({'features':train_1000, 'labels':train_1000_label}, file)\n",
    "\n",
    "with open('feret_test_50.pickle', 'wb') as file:\n",
    "    pickle.dump({'features':test_50, 'labels':test_50_label}, file)\n",
    "with open('feret_test_500.pickle', 'wb') as file:\n",
    "    pickle.dump({'features':test_500, 'labels':test_500_label}, file)\n",
    "with open('feret_test_1000.pickle', 'wb') as file:\n",
    "    pickle.dump({'features':test_1000, 'labels':test_1000_label}, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load COLOR FERET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "os.chdir('/home/huy/data/face-dataset/colorferet')\n",
    "with open('feret_train_50.pickle', 'rb') as file:\n",
    "    train_pickle = pickle.load(file)\n",
    "    train_50 = train_pickle['features']\n",
    "    train_50_label = train_pickle['labels']\n",
    "with open('feret_test_50.pickle', 'rb') as file:\n",
    "    test_pickle = pickle.load(file)\n",
    "    test_50 = test_pickle['features']\n",
    "    test_50_label = test_pickle['labels']\n",
    "    \n",
    "with open('feret_train_500.pickle', 'rb') as file:\n",
    "    train_pickle = pickle.load(file)\n",
    "    train_500 = train_pickle['features']\n",
    "    train_500_label = train_pickle['labels']\n",
    "with open('feret_test_500.pickle', 'rb') as file:\n",
    "    test_pickle = pickle.load(file)\n",
    "    test_500 = test_pickle['features']\n",
    "    test_500_label = test_pickle['labels']\n",
    "    \n",
    "with open('feret_train_1000.pickle', 'rb') as file:\n",
    "    train_pickle = pickle.load(file)\n",
    "    train_1000 = train_pickle['features']\n",
    "    train_1000_label = train_pickle['labels']\n",
    "with open('feret_test_1000.pickle', 'rb') as file:\n",
    "    test_pickle = pickle.load(file)\n",
    "    test_1000 = test_pickle['features']\n",
    "    test_1000_label = test_pickle['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slipt 1v1\n",
    "# COLOR FERET dataset\n",
    "os.chdir('/home/huy/data/face-dataset/colorferet')\n",
    "with open('feret_train_1v1.pickle', 'rb') as file:\n",
    "    train_pickle = pickle.load(file)\n",
    "    train_set = train_pickle['features']\n",
    "    train_set_label = train_pickle['labels']\n",
    "with open('feret_test_1v1.pickle', 'rb') as file:\n",
    "    test_pickle = pickle.load(file)\n",
    "    test_set = test_pickle['features']\n",
    "    test_set_label = test_pickle['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('feret_train_1v1.pickle', 'wb') as file:\n",
    "    pickle.dump({'features':train_set, 'labels':train_set_label}, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('feret_test_1v1.pickle', 'wb') as file:\n",
    "    pickle.dump({'features':test_set, 'labels':test_set_label}, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataset(features, labels):\n",
    "    vessel = list(zip(features, labels))\n",
    "    random.shuffle(vessel)\n",
    "    features, labels = zip(*vessel)\n",
    "    return list(features), list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_100, test_100_label = shuffle_dataset(test_100, test_100_label)\n",
    "test_500, test_500_label = shuffle_dataset(test_500, test_500_label)\n",
    "test_1000, test_1000_label = shuffle_dataset(test_1000, test_1000_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for AT&T dataset only\n",
    "train_label = train_set_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def predict(model, _train_set_label, sample, **hyperpara):\n",
    "    unknown_class_name = 'unknown'\n",
    "    raw = face_recognition.face_distance(model, sample)\n",
    "    min_distance = np.amin(raw)\n",
    "    min_index = np.argmin(raw)\n",
    "    if min_distance > hyperpara['tolerance']:\n",
    "        return unknown_class_name\n",
    "    return _train_set_label[min_index]\n",
    "\n",
    "def evaluate_overall(_test_set, _test_label, _train_set, _train_set_label, tolerance=0.5, detail=False):\n",
    "    test_pred = []\n",
    "    number_of_sample = 0\n",
    "    match = 0\n",
    "    not_match = 0\n",
    "    confusing = 0\n",
    "    if detail == True: \n",
    "        for i, sample in enumerate(_test_set):\n",
    "            raw_result = face_recognition.compare_faces(_train_set, sample, tolerance=tolerance)\n",
    "            if raw_result.count(True) > 1:\n",
    "                confusing += 1\n",
    "            else:\n",
    "                try:\n",
    "                    index = raw_result.index(True)\n",
    "                    if _train_set_label[index] == _test_set_label[i]:\n",
    "                        match += 1\n",
    "                    else:\n",
    "                        not_match += 1\n",
    "                except:\n",
    "                    not_match += 1\n",
    "            number_of_sample += 1\n",
    "    else:\n",
    "        for i, sample in enumerate(_test_set):\n",
    "            if predict(_train_set, _train_set_label, sample, tolerance=tolerance) == _test_set_label[i]:\n",
    "                match += 1\n",
    "            else:\n",
    "                not_match += 1\n",
    "            number_of_sample += 1\n",
    "    print('Number of samples: ', number_of_sample)\n",
    "    print('Confusing: ', confusing)\n",
    "    print('Match: ', match)\n",
    "    print('Not match: ', not_match)\n",
    "    return match/number_of_sample\n",
    "def evaluate_confusion_matrix(_test_set, _test_set_label, _train_set, _train_set_label, tolerance=0.5):\n",
    "    global count\n",
    "    global elapsed_sum\n",
    "    test_pred = []\n",
    "    labels = []\n",
    "    for label in _test_set_label:\n",
    "        if label not in labels:\n",
    "            labels.append(label)\n",
    "    for i, sample in enumerate(_test_set):\n",
    "        count += 1\n",
    "        t = time()\n",
    "        pred_label = predict(_train_set, _train_set_label, sample, tolerance=tolerance)\n",
    "        elapsed_sum += time() - t\n",
    "        test_pred.append(pred_label)\n",
    "    return confusion_matrix(_test_set_label, test_pred, labels=labels), test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test /home/huy/data/face-dataset/colorferet/01005/01005_960627_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/01005/01005_960627_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/01005/01005_960627_hl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00392/00392_940422_pl_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00392/00392_940422_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00392/00392_940422_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00392/00392_940422_pr_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00477/00477_940519_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00477/00477_940519_hl_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00477/00477_940519_pl_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00477/00477_940519_hl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00477/00477_940519_pr_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00477/00477_940519_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00370/00370_940422_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00370/00370_940422_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00146/00146_931230_pr_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00146/00146_960620_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00146/00146_960530_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00146/00146_960530_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00146/00146_941121_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00146/00146_960620_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00146/00146_960530_re.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00146/00146_931230_pl_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00146/00146_960627_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00146/00146_941205_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00146/00146_960627_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00146/00146_960620_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00146/00146_960530_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00146/00146_941201_re.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00146/00146_960627_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00146/00146_941201_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00146/00146_941121_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00146/00146_941205_hl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00146/00146_960620_re.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00146/00146_941201_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00146/00146_960627_hl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00146/00146_941205_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00813/00813_941205_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00813/00813_941205_hl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00070/00070_941121_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00070/00070_940422_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00070/00070_931230_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00070/00070_941031_rd.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00070/00070_941031_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00070/00070_940422_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00070/00070_940928_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00070/00070_940307_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00070/00070_941031_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00070/00070_931230_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00070/00070_941031_re.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00070/00070_941121_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00070/00070_940422_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00070/00070_940928_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00070/00070_940928_hl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00070/00070_941031_hl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00070/00070_940307_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00070/00070_940928_re.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00070/00070_941031_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00022/00022_930831_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00514/00514_940519_pr_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00514/00514_940519_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00988/00988_960627_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00988/00988_960627_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00988/00988_960627_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00988/00988_960627_re.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00016/00016_930831_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00334/00334_940422_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00334/00334_940422_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00484/00484_940519_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00457/00457_940422_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00457/00457_940422_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00965/00965_960627_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00965/00965_960627_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00239/00239_940128_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00239/00239_940128_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00503/00503_940519_pr_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00503/00503_940519_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00503/00503_940519_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00503/00503_940519_hr_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00503/00503_940519_pl_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00002/00002_940422_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00002/00002_940128_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00002/00002_940128_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00002/00002_931230_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00002/00002_940422_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00002/00002_940928_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00002/00002_940928_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00002/00002_931230_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00002/00002_930831_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00454/00454_940422_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00454/00454_940422_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00454/00454_940422_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00508/00508_960627_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00508/00508_940519_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00508/00508_940519_pr_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00508/00508_960627_hl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00508/00508_960627_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00508/00508_960627_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00508/00508_940519_pl_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00508/00508_940519_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00928/00928_960627_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00928/00928_960627_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00928/00928_960627_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00467/00467_940519_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00467/00467_940519_hl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00467/00467_940519_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00531/00531_960627_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00531/00531_960627_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00531/00531_960627_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00607/00607_940928_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00607/00607_940928_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00842/00842_940307_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00882/00882_960530_re.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00882/00882_960530_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00882/00882_960530_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00882/00882_960530_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00768/00768_960530_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00768/00768_941201_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00768/00768_941205_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00768/00768_941201_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00768/00768_960530_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00768/00768_960530_qr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00768/00768_960530_hl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00768/00768_960530_rd.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00768/00768_941201_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00768/00768_960530_re.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00768/00768_941201_re.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00768/00768_960530_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00768/00768_941205_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00887/00887_960530_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00887/00887_960530_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00887/00887_960530_re.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00066/00066_931230_pr_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00066/00066_931230_pl_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00155/00155_940128_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00155/00155_931230_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00155/00155_940128_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00155/00155_931230_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00043/00043_931230_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00043/00043_931230_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00243/00243_940128_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00243/00243_940128_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00544/00544_940519_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00544/00544_940519_pl_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00544/00544_940519_pr_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00544/00544_940519_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00981/00981_960627_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00981/00981_960627_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00981/00981_960627_hl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00981/00981_960627_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00729/00729_941205_ra.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00729/00729_941205_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00729/00729_941201_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00729/00729_941201_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00729/00729_941205_hl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00729/00729_941205_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00729/00729_941201_hl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00729/00729_941205_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00729/00729_941201_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00572/00572_940928_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00572/00572_940928_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00572/00572_940928_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00816/00816_960620_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00816/00816_960620_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00816/00816_960530_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00816/00816_960620_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00816/00816_941205_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00816/00816_941205_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00816/00816_960530_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00816/00816_941205_hl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00715/00715_941201_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00715/00715_941201_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00459/00459_940422_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00459/00459_940422_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00107/00107_941031_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00107/00107_931230_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00107/00107_941121_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00107/00107_941121_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00107/00107_941031_re.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00107/00107_941031_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00107/00107_931230_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00107/00107_941121_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00132/00132_931230_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00491/00491_960627_hl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00491/00491_960627_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00491/00491_960627_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00491/00491_960627_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00491/00491_960627_re.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00998/00998_960627_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00998/00998_960627_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00998/00998_960627_re.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00998/00998_960627_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00998/00998_960627_hl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00087/00087_931230_pr_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00087/00087_931230_pl_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00952/00952_960627_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00952/00952_960627_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00952/00952_960627_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00952/00952_960627_hl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00952/00952_960627_rd.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00115/00115_931230_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00115/00115_931230_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00755/00755_941201_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00755/00755_941201_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00755/00755_941201_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00755/00755_941201_hl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00755/00755_941201_re.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00179/00179_931230_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00179/00179_931230_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00976/00976_960627_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00976/00976_960627_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00976/00976_960627_re.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00113/00113_931230_pr_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00113/00113_931230_pl_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00692/00692_941121_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00692/00692_941121_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00971/00971_960627_re.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00971/00971_960627_rd.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00971/00971_960627_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00971/00971_960627_hl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00971/00971_960627_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00971/00971_960627_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00440/00440_940422_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00915/00915_960620_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00915/00915_960620_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00719/00719_960620_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00719/00719_960620_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00719/00719_941201_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00313/00313_940422_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00313/00313_940422_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00977/00977_960627_hl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00977/00977_960627_re.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00977/00977_960627_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00977/00977_960627_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00977/00977_960627_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00499/00499_940519_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00203/00203_940128_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00203/00203_940128_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00678/00678_941121_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00836/00836_940307_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00836/00836_940307_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00844/00844_940307_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00738/00738_941201_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00775/00775_960530_re.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00775/00775_960530_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00775/00775_960530_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00775/00775_960530_rd.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00775/00775_960530_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00775/00775_941205_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00775/00775_941205_hl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00775/00775_941205_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00549/00549_940519_pr_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00549/00549_940519_pl_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00163/00163_931230_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00163/00163_931230_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00327/00327_940422_hr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00327/00327_940422_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00327/00327_940422_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00823/00823_940307_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00823/00823_940307_pl_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00353/00353_940422_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00353/00353_940422_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00294/00294_940422_hr_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00294/00294_940422_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00294/00294_940422_pr_a.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00224/00224_940128_pl.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00224/00224_940128_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00385/00385_940422_pr.ppm\n",
      "Test /home/huy/data/face-dataset/colorferet/00385/00385_940422_pl.ppm\n"
     ]
    }
   ],
   "source": [
    "# test speed\n",
    "import glob\n",
    "image_lst = glob.glob('/home/huy/data/face-dataset/colorferet/**/*.ppm')\n",
    "count = 0\n",
    "total_time = 0\n",
    "for img_path in image_lst:\n",
    "    if count > 1000:\n",
    "        break\n",
    "    try:\n",
    "        t = time()\n",
    "        im = face_recognition.load_image_file(img_path)\n",
    "        encoded_face = face_recognition.face_encodings(im)[0]\n",
    "        predict(train_1000, train_1000_label,encoded_face, tolerance=1)\n",
    "    except:\n",
    "        print('Test', img_path)\n",
    "    finally:\n",
    "        total_time += time() - t\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19666094999094227"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_time/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9463414634146341\n",
      "0.9682249322493225\n",
      "0.9463414634146341\n",
      "0.9499601756445373\n",
      "7.60252882794636e-05\n",
      "\n",
      "0.9211150652431791\n",
      "0.9490058824133557\n",
      "0.9211150652431791\n",
      "0.9230482584044262\n",
      "0.0004886807354602384\n",
      "\n",
      "0.9059679767103348\n",
      "0.9423804496806206\n",
      "0.9059679767103348\n",
      "0.9104766407846276\n",
      "0.0009659419413737334\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huy/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/huy/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/huy/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/huy/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "\n",
    "threshold = 1\n",
    "unknown_class_name = 'unknown'\n",
    "# all_train_set = [TRAIN_SET for _ in range(5)]\n",
    "# all_train_label = [TRAIN_LABEL for _ in range(5)]\n",
    "# all_test_set = [TEST_SET, TEST_SET_LEFT, TEST_SET_RIGHT, TEST_SET_UP, TEST_SET_DOWN]\n",
    "# all_test_label = [TEST_LABEL, TEST_LEFT_LABEL, TEST_RIGHT_LABEL, TEST_UP_LABEL, TEST_DOWN_LABEL]\n",
    "\n",
    "all_test_set = [test_50, test_500, test_1000]\n",
    "all_test_label = [test_50_label, test_500_label, test_1000_label]\n",
    "all_train_set = [train_50, train_500, train_1000]\n",
    "all_train_label = [train_50_label, train_500_label, train_1000_label]\n",
    "\n",
    "# all_test_set = [TEST_SET_DOWN]\n",
    "# all_test_label = [TEST_DOWN_LABEL]\n",
    "# all_train_set = [TRAIN_SET]\n",
    "# all_train_label = [TRAIN_LABEL]\n",
    "\n",
    "# all_test_set = TEST_SET_LIST\n",
    "# all_test_label = TEST_LABEL_LIST\n",
    "# all_train_set = [TRAIN_SET]*5\n",
    "# all_train_label = [TRAIN_LABEL]*5\n",
    "\n",
    "for test_set, test_set_label, train_set, train_set_label in zip(all_test_set, all_test_label, all_train_set, all_train_label):\n",
    "#     evaluate_overall(threshold, detail=True)\n",
    "    count = 0\n",
    "    elapsed_sum = 0\n",
    "    cfmx, test_pred  = evaluate_confusion_matrix(test_set, test_set_label, train_set, train_set_label, tolerance=threshold)\n",
    "#     print ('ACC', sklearn.metrics.accuracy_score(test_set_label, test_pred))\n",
    "#     print ('PRE', sklearn.metrics.precision_score(test_set_label, test_pred, average='macro'))\n",
    "#     print ('RECALL', sklearn.metrics.recall_score(test_set_label, test_pred, average='macro'))\n",
    "#     print ('F1_SCORE', sklearn.metrics.f1_score(test_set_label, test_pred, average='macro'))\n",
    "    labels = []\n",
    "    for label in train_set_label:\n",
    "        if label not in labels:\n",
    "            labels.append(label)\n",
    "    recall = sklearn.metrics.recall_score(test_set_label, test_pred, labels=labels, average='weighted')\n",
    "    precision = sklearn.metrics.precision_score(test_set_label, test_pred, labels=labels, average='weighted')\n",
    "    accuracy = sklearn.metrics.accuracy_score(test_set_label, test_pred)\n",
    "    f1_score = sklearn.metrics.f1_score(test_set_label, test_pred, labels=labels, average='weighted')\n",
    "    print(recall)\n",
    "    print(precision)\n",
    "    print(accuracy)\n",
    "    print(f1_score)\n",
    "    print(elapsed_sum/count)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/huy/data/face-dataset/ExtendedYaleB/yaleB39/yaleB39_P00A+000E+00.pgm'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_SET[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINDING THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "max_acc = 0\n",
    "good_tolerance = 0\n",
    "for tolerance in np.linspace(0, 1, 100):\n",
    "    acc = evaluate_overall(tolerance)\n",
    "    if acc > max_acc:\n",
    "        good_tolerance = tolerance\n",
    "        max_acc = acc\n",
    "print(\"Max acc: \", max_acc)\n",
    "print(\"Tolerance: \", good_tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "skplt.metrics.plot_confusion_matrix(test_set_label, test_pred, normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Recognition Evaluation More Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AT&T Face database && FERET database\n",
    "# Testing Model performance with different face poses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attr id\n",
    "center 0\n",
    "halfleft 1\n",
    "halfright 2\n",
    "headup 3\n",
    "headdown 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AT face dataset\n",
    "os.chdir('/home/huy/data/face-dataset/att_faces/orl_faces_labeled')\n",
    "\n",
    "# test_attr_labels = []\n",
    "\n",
    "# train_set = []\n",
    "# train_label = []\n",
    "\n",
    "# test_set = []\n",
    "# test_label = []\n",
    "\n",
    "# attr_labels = ['center', 'halfleft', 'halfright', 'headup', 'headdown']\n",
    "\n",
    "# class_number = 0\n",
    "\n",
    "# for entry in os.scandir('.'):\n",
    "#     if entry.is_dir():\n",
    "#         if len(entry.name) <= 3:\n",
    "#             enough_data = False\n",
    "#             for image_entry in os.scandir(entry.path):\n",
    "#                 for index, value in enumerate(attr_labels):\n",
    "#                     if value in image_entry.name:\n",
    "#                         image = face_recognition.load_image_file(image_entry.path)\n",
    "#                         h, w, _ = image.shape\n",
    "#                         roi = [(0,w,h,0)]\n",
    "#                         encoded = face_recognition.face_encodings(image, known_face_locations=roi)[0]\n",
    "#                         if image_entry.name.startswith('center_1'):\n",
    "#                             if not enough_data:\n",
    "#                                 train_set.append(encoded)\n",
    "#                                 train_label.append(class_number)\n",
    "#                                 enough_data = True\n",
    "#                         else:\n",
    "#                             test_set.append(encoded)\n",
    "#                             test_label.append(class_number)\n",
    "#                             test_attr_labels.append(index)\n",
    "#             class_number += 1\n",
    "# attr_group = []\n",
    "# for i,v in enumerate(test_attr_labels):\n",
    "#     if v not in [x[2] for x in attr_group]:\n",
    "#         attr_group.append([[test_set[i]], [test_label[i]], v])\n",
    "#     else:\n",
    "#         index = [x[2] for x in attr_group].index(v)\n",
    "#         attr_group[index][0].append(test_set[i])\n",
    "#         attr_group[index][1].append(test_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_class_number = max([int(x) for x in train_label]) + 1\n",
    "def predict(model, sample, **hyperpara):\n",
    "    raw = face_recognition.face_distance(model, sample)\n",
    "    min_distance = np.amin(raw)\n",
    "    min_index = np.argmin(raw)\n",
    "    if min_distance > hyperpara['tolerance']:\n",
    "        return unknown_class_number\n",
    "    return train_label[min_index]\n",
    "\n",
    "def evaluate(test_set, true_label, tolerance=0.5):\n",
    "    test_pred = []\n",
    "    for i, sample in enumerate(test_set):\n",
    "        test_pred.append(predict(train_set, sample, tolerance=tolerance))\n",
    "    return test_pred\n",
    "\n",
    "def evaluate_confusion_matrix(test_set, true_label, tolerance=0.5):\n",
    "    for i, sample in enumerate(test_set):\n",
    "        test_pred.append(predict(train_set, sample, tolerance=tolerance))\n",
    "    return confusion_matrix(true_label, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = []\n",
    "for i in range(0,1):\n",
    "    test_pred = []\n",
    "    test_set, test_label, _ = attr_group[i]\n",
    "    test_pred = evaluate(test_set, test_label, 0.58)\n",
    "    report.append(sklearn.metrics.classification_report(test_label, test_pred, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_acc(test_label, test_pred):\n",
    "    max_class_number = max(test_pred)\n",
    "    class_samples = [0 for i in range(0,max_class_number+1)]\n",
    "    true_pred = [0 for i in range(0,max_class_number+1)]\n",
    "    for i,v in enumerate(test_pred):\n",
    "        try:\n",
    "            if test_label[i] == v:\n",
    "                true_pred[int(v)] += 1\n",
    "            class_samples[int(v)] += 1\n",
    "        except IndexError:\n",
    "            print(i,v)\n",
    "    return [x/y for x,y in zip(true_pred,class_samples) if y > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for visual part\n",
    "\n",
    "std_accs = []\n",
    "std_f1s = []\n",
    "for i in range(0,5):\n",
    "    test_label = attr_group[i][1]\n",
    "    test_pred = []\n",
    "    for index, sample in enumerate(attr_group[i][0]):\n",
    "        test_pred.append(predict(train_set, sample, tolerance=0.58))\n",
    "    report = sklearn.metrics.classification_report(test_label, test_pred, output_dict=True)\n",
    "    f1s = []\n",
    "    for label in range(0,31):\n",
    "        if str(label) in report.keys():\n",
    "            print(report[str(label)]['f1-score'])\n",
    "            f1s.append(report[str(label)]['f1-score'])\n",
    "    accs = evaluate_acc(test_label, test_pred)\n",
    "    std_accs.append(np.std(accs))\n",
    "    std_f1s.append(np.std(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Threshold\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "training_report = {\n",
    "    'precision' : [],\n",
    "    'recall' : [],\n",
    "    'threshold' : []\n",
    "}\n",
    "def training():   \n",
    "    max_f1_score = 0\n",
    "    good_tolerance = 0\n",
    "    for tolerance in np.linspace(0, 1, 500):\n",
    "        test_pred = []\n",
    "        for i, sample in enumerate(test_set):\n",
    "            test_pred.append(predict(train_set, sample, tolerance=tolerance))\n",
    "        f1_score = sklearn.metrics.f1_score(test_label, test_pred, average='samples')\n",
    "        precision = sklearn.metrics.precision_score(test_label, test_pred, average='samples')\n",
    "        recall = sklearn.metrics.recall_score(test_label, test_pred, average='samples')\n",
    "        training_report['precision'].append(precision)\n",
    "        training_report['recall'].append(recall)\n",
    "        training_report['threshold'].append(tolerance)\n",
    "        if f1_score > max_f1_score:\n",
    "            good_tolerance = tolerance\n",
    "            max_f1_score = f1_score\n",
    "#     print(\"Max f1 score: \", max_f1_score)\n",
    "#     print(\"Tolerance: \", good_tolerance)\n",
    "    return good_tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(training_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.to_excel(\"data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Evaluation Result\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "from scipy.signal import savgol_filter\n",
    "plt.style.use('grayscale')\n",
    "\n",
    "# x = np.linspace(0,1, 100)\n",
    "# spl1 = make_interp_spline(training_report['threshold'], training_report['precision'], k=9)\n",
    "# spl2 = make_interp_spline(training_report['threshold'], training_report['recall'], k=9)\n",
    "# y1 = spl1(x)\n",
    "# y2 = spl2(x)\n",
    "x = training_report['threshold']\n",
    "y1 = savgol_filter(training_report['precision'], 51, 3)\n",
    "y2 = savgol_filter(training_report['recall'], 51, 3)\n",
    "line1 = plt.plot(x, y1, label='precision')\n",
    "line2 = plt.plot(x, y2, label='recall')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "for i in range(0,5):\n",
    "    test_set = attr_group[i][0]\n",
    "    test_label = attr_group[i][1]\n",
    "    training()\n",
    "    test_pred = []\n",
    "    evaluate_confusion_matrix(test_set, test_label, good_tolerance);\n",
    "    print(attr_labels[i])\n",
    "    print('TOTAL: ', test_attr_labels.count(i))\n",
    "    print ('ACC: ', sklearn.metrics.accuracy_score(test_label, test_pred))\n",
    "    print ('PRECISION: ', sklearn.metrics.precision_score(test_label, test_pred, average='macro'))\n",
    "    print ('RECALL: ', sklearn.metrics.recall_score(test_label, test_pred, average='macro'))\n",
    "    print ('F1 SCORE: ', sklearn.metrics.f1_score(test_label, test_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_scores = [85.18, 75.17, 90.32, 92.10, 76.47]\n",
    "f1_scores = [80.00, 73.06, 82.45, 84.00, 60.42]\n",
    "face_pose = ['Center', 'Half left', 'Half right', 'Head up', 'Head down']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_scores = [99.03, 94.55, 93.62, 98.81, 92.31]\n",
    "f1_scores = [98.36, 92.64, 89.04, 98.85, 83.91]\n",
    "face_pose = ['Center', 'Half left', 'Half right', 'Head up', 'Head down']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(zip(acc_scores, f1_scores, face_pose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [x[1], x[4], x[0], x[2], x[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.sort(key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_scores, f1_scores, face_pose = zip(*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use('fivethirtyeight')\n",
    "plt.style.use('classic')\n",
    "# plt.style.use('grayscale')\n",
    "\n",
    "\n",
    "# acc_std_percentage = [x*50 for x in std_accs]\n",
    "# f1_std_percentage = [x*50 for x in std_f1s]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig\n",
    "bar_width = 0.25\n",
    "x1 = np.arange(len(acc_scores))\n",
    "x2 = [x + bar_width for x in x1]\n",
    "# ax.bar(x1, acc_scores, yerr=acc_std_percentage, width=bar_width, alpha=0.5 , label='Accuracy')\n",
    "# ax.bar(x2, f1_scores, yerr=f1_std_percentage, width=bar_width, alpha=0.5, label='F1-score')\n",
    "ax.set_ylim([0,119])\n",
    "ax.bar(x1, acc_scores, width=bar_width, alpha=0.5 , label='Accuracy')\n",
    "ax.bar(x2, f1_scores, width=bar_width, alpha=0.5, label='F1-score')\n",
    "ax.set_ylabel(\"Percentage (%)\")\n",
    "ax.set_xticks([r + bar_width for r in range(len(acc_scores))])\n",
    "ax.set_xticklabels(face_pose)\n",
    "ax.yaxis.grid(True)\n",
    "ax.set_xlabel(\"Face Poses\")\n",
    "for _x1, _y1, _x2, _y2 in zip(x1,acc_scores,x2,f1_scores):\n",
    "    ax.text(_x1, _y1 + 2, str(_y1), rotation='horizontal', horizontalalignment='center', verticalalignment='center')\n",
    "    ax.text(_x2, _y2 + 2, str(_y2), rotation='horizontal', horizontalalignment='center', verticalalignment='center')\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(len(acc_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pfiev_model.pickle', 'wb') as file:\n",
    "    pickle.dump({'features':TRAIN_SET, 'labels':TRAIN_LABEL}, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pfiev_model.pickle', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = ['person1', 'person2']\n",
    "actual = ['person1','person1']\n",
    "sklearn.metrics.precision_score(predict, actual, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
